{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4eb82e-6626-4f2d-a4a3-f4b4d7bf2265",
   "metadata": {},
   "source": [
    "### Lab 5: Processing Incremental Updates with PySpark Structured Streaming and Delta tables\n",
    "In this lab you'll apply your knowledge of PySpark and structured streaming to implement a simple multi-hop (medallion) architecture.\n",
    "\n",
    "#### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a49db51-9b2b-4a76-a6a8-a0c81a910fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4cb0a2-a6e0-4645-a4cd-c7fd5aa2458b",
   "metadata": {},
   "source": [
    "#### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4b48bd-6d42-44ee-8f3e-f760be729f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'retail-org')\n",
    "customers_stream_dir = os.path.join(data_dir, 'customers')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"customers_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "customers_output_bronze = os.path.join(database_dir, 'customers_bronze')\n",
    "customers_output_silver = os.path.join(database_dir, 'customers_silver')\n",
    "customers_output_gold = os.path.join(database_dir, 'customers_gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96fc91-2301-484f-8221-5d1f020b556a",
   "metadata": {},
   "source": [
    "#### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99ea358-3f4c-45ed-834e-e04eaf8b5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b153a81-c29c-4d19-9166-93c52bec80c0",
   "metadata": {},
   "source": [
    "#### 4.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03439397-c71e-475e-969f-bc8fcf1a0763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 23:45:41 WARN Utils: Your hostname, Sonias-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.26.76.137 instead (on interface en0)\n",
      "25/04/07 23:45:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/soniasadani/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/soniasadani/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a597ecf5-b5fa-4bb5-af2d-ad8aedd13195;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 79ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a597ecf5-b5fa-4bb5-af2d-ad8aedd13195\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/04/07 23:45:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.26.76.137:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Customers Delta Table in Juptyer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11ff07560>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('PySpark Customers Delta Table in Juptyer')\\\n",
    "    .master(worker_threads)\\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '2g')\\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .config('spark.sql.debug.maxToStringFields', 50) \\\n",
    "    .config('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .config('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .config('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .config('spark.sql.warehouse.dir', database_dir) \\\n",
    "    .config('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb88351-4088-4bc5-9261-d17020bd1a69",
   "metadata": {},
   "source": [
    "### 5.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d5c0a53-9940-41b6-9f3b-159fce1f3c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/soniasadani/Documents/04-PySpark/spark-warehouse/customers_dlh' has been removed successfully.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a740574-00a0-4f0e-94f7-a2e682836fa1",
   "metadata": {},
   "source": [
    "#### 6.0. Bronze Table: Ingest and Stage Data\n",
    "This lab uses a collection of customer-related CSV data found in *`../04-PySpark/lab_data/retail-org/customers/`*. \n",
    "<br>This is available to you by way of the `customers_stream_dir` variable.\n",
    "\n",
    "##### 6.1. Read this data into a Stream using schema inference\n",
    "- Use a **`_checkpoint`** folder and the **`schemaLocation`** option to store the schema info in a dedicated folder for **`customers`**.\n",
    "- Set the **`maxFilesPerTrigger`** option to **`1`**.\n",
    "- Set the **`inferSchema`** and **`header`** options to **`true`**.\n",
    "- Use **`.csv()`** to specify the source directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac7fb54-d655-4b5f-8af3-571dd4cc65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_checkpoint_bronze = os.path.join(customers_output_bronze, '_checkpoint')\n",
    "\n",
    "df_customers_bronze = (\n",
    "    spark.readStream \\\n",
    "    .format(\"csv\")\n",
    "        .option(\"schemaLocation\", customers_checkpoint_bronze)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(customers_stream_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0867da-7a59-4c59-ae54-7500b6716dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- tax_id: double (nullable = true)\n",
      " |-- tax_code: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- ship_to_address: string (nullable = true)\n",
      " |-- valid_from: integer (nullable = true)\n",
      " |-- valid_to: double (nullable = true)\n",
      " |-- units_purchased: double (nullable = true)\n",
      " |-- loyalty_segment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce9616-7a4f-4599-9415-27c5148bd437",
   "metadata": {},
   "source": [
    "##### 6.2. Stream the raw data to a Delta table.\n",
    " - Use the **`delta`** format.\n",
    " - Use the **`append`** output mode.\n",
    " - Use **`customers_bronze`** as the **`queryName`**.\n",
    " - Use **`availableNow = True`** for the **`trigger`**\n",
    " - Use the **`_checkpoint`** folder with the **`checkpointLocation`** option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83953cd-473b-458d-a291-fa4c643ba95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dest_database}\")\n",
    "customers_bronze_query = (\n",
    "    df_customers_bronze\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .queryName(\"customers_bronze\")\n",
    "        .trigger(availableNow=True)\n",
    "        .option(\"checkpointLocation\", customers_checkpoint_bronze)\n",
    "        .toTable(f\"{dest_database}.customers_bronze\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e13f595e-6f24-4bf2-9539-9c3ebffe8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d8ffa7d-667c-44e1-ad99-9f79f849136e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customers.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(customers_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207737cb-bce8-4dd1-97f9-fcb7433e2a07",
   "metadata": {},
   "source": [
    "##### 6.3. Create a Streaming Temporary View named **`customers_bronze_temp`**\n",
    "- Use the **`delta`** format.\n",
    "- Set the **`inferSchema`** option to **`true`**\n",
    "- Load the data from the output of the **`bronze`** delta table (**`customers_output_bronze`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfaf84fe-cb95-47b5-af23-d976884eb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dest_database}\")\n",
    "\n",
    "customers_bronze_query = (\n",
    "    df_customers_bronze\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .queryName(\"customers_bronze\")\n",
    "        .trigger(availableNow=True)\n",
    "        .option(\"checkpointLocation\", customers_checkpoint_bronze)\n",
    "        .toTable(f\"{dest_database}.customers_bronze\")\n",
    ")\n",
    "\n",
    "customers_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb56a9-294f-4ecc-8c99-97504cd94e76",
   "metadata": {},
   "source": [
    "##### 6.4. Clean and Enhance the Data\n",
    "Use the CTAS syntax to define a new streaming view called **`bronze_enhanced_temp`** that does the following:\n",
    "* Omits records with a null **`postcode`** (set to zero)\n",
    "* Inserts a column called **`receipt_time`** containing a current timestamp using the **`current_timestamp()`** function.\n",
    "* Inserts a column called **`source_file`** containing the input filename using the **`imput_file_name()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25fd45bf-3b1f-4095-a1e4-0c6e42a959b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.readStream\n",
    "        .table(f\"{dest_database}.customers_bronze\")\n",
    "        .createOrReplaceTempView(\"customers_bronze_temp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbb435-80aa-4861-a7b9-e8c4c7110512",
   "metadata": {},
   "source": [
    "#### 7.0. Silver Table\n",
    "##### 7.1. Stream the data from **`bronze_enhanced_temp`** to a **`Delta`** table named **`customers_silver`**.\n",
    " - Use the **`append`** output mode.\n",
    " - Use **`customers_silver`** as the **`queryName`**.\n",
    " - Use **`availableNow = True`** for the **`trigger`**\n",
    " - Use a **`_checkpoint`** folder with the **`checkpointLocation`** option to store the schema info in a dedicated folder for **`customers`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2269fde-af1d-47fd-947d-591f9d1a6083",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_enhanced_temp` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [bronze_enhanced_temp], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m customers_silver_query \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM bronze_enhanced_temp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomers_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39mtrigger(availableNow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, customers_checkpoint_silver)\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mtoTable(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest_database\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.customers_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m customers_silver_query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_enhanced_temp` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [bronze_enhanced_temp], [], false\n"
     ]
    }
   ],
   "source": [
    "customers_silver_query = (\n",
    "    spark.sql(\"SELECT * FROM bronze_enhanced_temp\")\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .queryName(\"customers_silver\")\n",
    "        .trigger(availableNow=True)\n",
    "        .option(\"checkpointLocation\", customers_checkpoint_silver)\n",
    "        .toTable(f\"{dest_database}.customers_silver\")\n",
    ")\n",
    "\n",
    "customers_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36e583-5852-453c-a550-6afb3bcb8f08",
   "metadata": {},
   "source": [
    "##### 7.2. Create a Streaming Temporary View\n",
    "- Create another streaming temporary view named **`customers_silver_temp`** for the **`customers_silver`** table so we can perform business-level queries using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14241fe9-3949-4c74-8659-54562d38bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.readStream \\\n",
    "     # TODO: Confgurations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a890d-2d46-4d9b-a1f3-e017fe5482a6",
   "metadata": {},
   "source": [
    "#### 8.0. Gold Table\n",
    "##### 8.1. Use the CTAS syntax to define a new streaming view called **`customer_count_by_state_temp`** that does the following:\n",
    "- Reads data from the **`customers_silver_temp`** temporary view created in the preceding step.\n",
    "- Selects the **`state`** along with the number of customers per (grouped by) state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a981dcc-966e-4c9c-a064-aafde4d47f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_gold_temp = \"\"\"\n",
    "    TODO: Author SQL statement\n",
    "\"\"\"\n",
    "spark.sql(sql_gold_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01970d-07ac-41ab-a7d8-f1ebe00ddd48",
   "metadata": {},
   "source": [
    "##### 8.2. Stream the data from the **`customer_count_by_state_temp`** view to a Delta table called **`customer_count_by_state_gold`**.\n",
    "- Use the **`complete`** output mode because aggregations like **`count()`** and sorting cannot operate on *unbounded* datasets.  \n",
    "- Use a **`_checkpoint`** folder with the **`checkpointLocation`** option and a dedicated folder for **`customers`** as the checkpoint path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208a889-449f-43ad-af0b-526e29df0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_count_checkpoint_gold = os.path.join(customers_output_gold, '_checkpoint')\n",
    "\n",
    "customer_count_by_state_gold_query = \\\n",
    "(spark.table(\"customer_count_by_state_temp\") \\\n",
    "     # TODO: Configurations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d70171-85e5-4811-be9a-390c708ed36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_count_by_state_gold_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b5a3b-47ce-4436-b090-c081e749250c",
   "metadata": {},
   "source": [
    "#### 9.0. Query the Results\n",
    "- Query the **`customer_count_by_state_gold`** table (this will not be a streaming query).\n",
    "- Select the **`state`** and **`customer_count`** columns.\n",
    "- Sort the results by **`customer_count`** in descending order (i.e., from highest to lowest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ca43a-2faf-41b2-8729-6e95db7fd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_customer_count_query = \"\"\"\n",
    "    TODO: Author SQL query\n",
    "\"\"\"\n",
    "spark.sql(sql_customer_count_query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d980d-76a4-4566-b376-18ab2e2a2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysparkenv]",
   "language": "python",
   "name": "conda-env-pysparkenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
